import re
import streamlit as st
import pandas as pd
import json
from datetime import datetime
from Component.ResumeAnalysis.resume_anaops_mysql import get_analy_json_by_analysis_id, get_resume_analysis
from Component.HR_ops.hr_insights_ops import (
    get_performance_by_user, save_performance,
    get_training_by_analysis, save_training,
    get_retention_by_user, save_retention
)
from Component.ResumeAnalysis.resume_processor import ResumeProcessor
from oputils.secret import decrypt_api_key
from oputils.config_utils import get_model_config
from zhipuapi import call_gpt_model
import Component as co

def parse_model_output(raw_str: str) -> dict:
    """
    å¤„ç†æ¨¡å‹è¿”å›çš„å¸¦æœ‰ ```json ... ``` çš„å­—ç¬¦ä¸²ï¼Œå»é™¤æ ‡è®°åè½¬æ¢æˆPythonå­—å…¸ã€‚
    """
    cleaned_str = re.sub(r"^```json\s*|```$", "", raw_str.strip(), flags=re.DOTALL)
    try:
        data = json.loads(cleaned_str)
    except Exception as e:
        st.error(f"è§£ææ¨¡å‹è¿”å›JSONå¤±è´¥ï¼š{e}")
        return {}
    return data
def hr_insights_page(phonenumber):
  st.title("ğŸš€ HR æ™ºèƒ½æ´å¯Ÿ")

  # 1. å…ˆè®©ç”¨æˆ·é€‰æ‰‹æœºå·ä¸‹çš„åˆ†æè®°å½•
  ph =  phonenumber
  if not ph:
      st.warning("è¯·å…ˆç™»å½•ã€‚")
      st.stop()
      
  resumes = co.get_user_resumes(phonenumber)
  if not resumes:
      st.info("æš‚æ— ç®€å†ï¼Œè¯·ä¸Šä¼ ã€‚")
      return

  resume_dict = {row[1]: row[0] for row in resumes}
  resume_name = st.selectbox("è¯·é€‰æ‹©è¦åˆ†æçš„ç®€å†", list(resume_dict.keys()))
  resume_id = resume_dict[resume_name]
  resume = co.get_resume_by_id(resume_id)
  # st.write(resume)
  resume_name = resume[2]
  resume_json = resume[6]

  analyses = co.get_resume_analysis(resume_id)  # æ”¹é€ ï¼šå¯æŒ‰æ‰‹æœºå·ç­›é€‰

  options = [
      f"ID:{record[0]} - åˆ†æ•°:{record[1]} - æ—¶é—´:{record[2].strftime('%Y-%m-%d %H:%M:%S')}"
      for record in analyses 
  ]
  selected_option = st.selectbox("é€‰æ‹©åˆ†æè®°å½•", options , index=0)
  analysis_id = int(selected_option.split(" ")[0].split(":")[1])
  analy_json = get_analy_json_by_analysis_id(analysis_id)
  job_name = analy_json["job_name"]
  # æ„é€ é€‰é¡¹ï¼šID - æ—¶é—´ - åˆ†æ•°
  options = [
      f"ID:{r[0]}  æ—¶é—´:{r[2].strftime('%Y-%m-%d %H:%M')}  åˆ†æ•°:{r[1]}"
      for r in analyses
  ]

  cfg = get_model_config()
  api_url = cfg["api_url"]
  api_key = cfg["api_key"]
  api_name = cfg["model_name"]
  # st.write(cfg)
  
  # â”€â”€ Tab 1: ç»©æ•ˆè¯„ä¼° â”€â”€
  tab1, tab2, tab3 = st.tabs(["ğŸ“ˆ ç»©æ•ˆè¯„ä¼°", "ğŸ¯ åŸ¹è®­æ¨è", "â¤ï¸ æ»¡æ„åº¦é¢„æµ‹"])
  prompt_template1 = """ä½ æ˜¯ä¸€ä¸ªä¼ä¸šäººåŠ›ç»©æ•ˆåˆ†æä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹å†…å®¹è¯„ä¼°å‘˜å·¥æœ¬æ¬¡çš„å·¥ä½œç»©æ•ˆæƒ…å†µã€‚

å²—ä½ï¼š{job_name}
å²—ä½åŒ¹é…åº¦å’ŒæŠ€èƒ½é€‚é…åº¦åˆ†æï¼š{analy_json}
å‘˜å·¥è¿‘æœŸå·¥ä½œæˆæœï¼ˆç”±ç”¨æˆ·å¡«å†™ï¼‰ï¼š{work_summary}

è¯·ä»ä»¥ä¸‹ç»´åº¦åˆ†åˆ«æ‰“åˆ†ï¼ˆ0-100ï¼‰ï¼š
1. å²—ä½åŒ¹é…åº¦ï¼ˆjob_fitï¼‰
2. æŠ€èƒ½é€‚é…åº¦ï¼ˆskill_matchï¼‰
3. æ²Ÿé€šä¸åä½œèƒ½åŠ›ï¼ˆcommunicationï¼‰

è¿”å›æ ‡å‡† JSON æ ¼å¼å¦‚ä¸‹ï¼š
{{
  "job_fit": 0,
  "skill_match": 0,
  "communication": 0,
  "summary": "è¯„ä¼°æ€»ç»“æ€§æè¿°"
}}
"""
  with tab1:
    st.subheader("å†å²ç»©æ•ˆè®°å½•")
    perf = get_performance_by_user(ph,resume_id)
    if perf:
        df = pd.DataFrame(perf)
        df["scores"] = df["scores"].map(lambda s: json.loads(s))
        st.dataframe(df[["created_at", "scores", "summary"]])
        st.line_chart(pd.DataFrame([
            {**json.loads(r["scores"]), "æ—¶é—´": r["created_at"]} for r in perf
        ]).set_index("æ—¶é—´"))
    else:
        st.info("æš‚æ— ç»©æ•ˆæ•°æ®ã€‚")

    work_summary = st.text_area("è¯·è¾“å…¥è¯¥å‘˜å·¥è¿‘æœŸçš„å·¥ä½œæˆæœï¼š", "")
    if st.button("ç”Ÿæˆæœ¬æ¬¡ç»©æ•ˆè¯„ä¼°") and work_summary.strip():
        prompt = prompt_template1.format(
            job_name=job_name,
            work_summary=work_summary,
            analy_json=analy_json
        )

        resp = call_gpt_model(prompt, api_name,api_url, api_key)
        try:
            data = parse_model_output(resp)
            scores = {
                "job_fit": data.get("job_fit", 0),
                "skill_match": data.get("skill_match", 0),
                "communication": data.get("communication", 0)
            }
            summary = data.get("summary", "ç³»ç»Ÿç”Ÿæˆæ€»ç»“")
            save_performance(ph, resume_id,analysis_id, scores, summary)
            st.success("å·²ä¿å­˜æœ¬æ¬¡ç»©æ•ˆè¯„ä¼°ã€‚")
            st.rerun()
        except Exception as e:
            st.error(f"ç»©æ•ˆè¯„ä¼°è§£æå¤±è´¥: {e}")

  prompt_template2 = """ä½ æ˜¯ä¼ä¸šåŸ¹è®­å¸ˆã€‚è¯·æ ¹æ®ä»¥ä¸‹å†…å®¹ï¼Œä¸ºå‘˜å·¥è®¾è®¡ä¸ªæ€§åŒ–åŸ¹è®­æ¨èã€‚

å²—ä½åç§°ï¼š{job_name}
ç®€å†å†…å®¹ï¼š{resume_text}
å²—ä½åŒ¹é…åˆ†æï¼š{analysis_result}

è¯·æ¨è 2-3 ä¸ªåŸ¹è®­ä¸»é¢˜ï¼Œå¹¶è¯´æ˜æ¯ä¸ªä¸»é¢˜çš„ç†ç”±ï¼Œè¿”å›ä»¥ä¸‹ JSON æ ¼å¼ï¼š
{{
  "recommendations": [
    {{
      "topic": "æ²Ÿé€šæŠ€å·§æå‡",
      "reason": "è¯¥å‘˜å·¥åœ¨æ²Ÿé€šèƒ½åŠ›æ–¹é¢å­˜åœ¨çŸ­æ¿ã€‚"
    }},
    {{
      "topic": "Python é«˜çº§ç¼–ç¨‹",
      "reason": "æŠ€æœ¯èƒ½åŠ›å¯è¿›ä¸€æ­¥åŠ å¼ºä»¥åŒ¹é…å²—ä½è¦æ±‚ã€‚"
    }}
  ]
}}
"""
  with tab2:
    st.subheader("æœ€æ–°åŸ¹è®­æ¨è")
    train = get_training_by_analysis(analysis_id)
    if train:
        st.markdown(train["content"])
    else:
        st.info("æš‚æ— åŸ¹è®­æ¨èã€‚")

    if st.button("ä¸€é”®ç”ŸæˆåŸ¹è®­æ–¹æ¡ˆ"):
        prompt = prompt_template2.format(
            job_name=job_name,
            resume_text=resume_json,
            analysis_result=json.dumps(analy_json, ensure_ascii=False)
        )
        resp = call_gpt_model(prompt, api_name,api_url, api_key)
        try:
            data = parse_model_output(resp)
            # print(data)
            md = "\n".join([f"- **{item['topic']}**ï¼š{item['reason']}" for item in data.get("recommendations", [])])
            save_training(ph,resume_id,analysis_id,md)
            st.success("å·²ç”Ÿæˆå¹¶ä¿å­˜åŸ¹è®­æ–¹æ¡ˆã€‚")
            st.rerun()
        except Exception as e:
            st.error(f"åŸ¹è®­æ¨èè§£æå¤±è´¥: {e}")

  prompt_template3 = """ä½ æ˜¯ä¸€ä¸ªäººåŠ›èµ„æºä¸“å®¶ï¼Œè¯·æ ¹æ®ä»¥ä¸‹å²—ä½åŒ¹é…åˆ†ææ•°æ®ï¼Œé¢„æµ‹è¯¥å‘˜å·¥çš„ç¦»èŒé£é™©ï¼Œå¹¶è¯´æ˜ç†ç”±ã€‚

åˆ†æç¼–å·ï¼š{analysis_id}
å²—ä½åç§°ï¼š{job_name}
å²—ä½åŒ¹é…åˆ†æç»“æœï¼ˆJSONï¼‰ï¼š{analysis_result}

è¯·è¿”å›æ ‡å‡† JSON æ ¼å¼ï¼š
{{
  "risk_level": "",  // å¯é€‰å€¼ï¼šä½ã€ä¸­ã€é«˜
  "reason": "" // ç¦»èŒé£é™©åŸå› 
}}
"""
  with tab3:
      st.subheader("ç¦»èŒé£é™©é¢„æµ‹")
      
      preds = get_retention_by_user(ph, resume_id)
      
      risk_map = {"é«˜": 3, "ä¸­": 2, "ä½": 1, "æœªçŸ¥": 0}

      if preds:
          df2 = pd.DataFrame(preds)
          df2["details"] = df2["details"].map(lambda s: json.loads(s) if s else {})
          df2["risk_score"] = df2["risk_level"].map(risk_map)
          df2["created_at"] = pd.to_datetime(df2["created_at"])

          # å±•ç¤ºæ•°æ®è¡¨
          st.dataframe(df2[["created_at", "risk_level", "details"]])

          # æŠ˜çº¿å›¾å±•ç¤ºé£é™©ç­‰çº§è¶‹åŠ¿
          st.markdown("#### ğŸ“ˆ é£é™©ç­‰çº§è¶‹åŠ¿å›¾")
          chart_data = df2[["created_at", "risk_score"]].set_index("created_at").sort_index()
          st.line_chart(chart_data)

      else:
          st.info("æš‚æ— é¢„æµ‹æ•°æ®ã€‚")

      # ç”Ÿæˆé¢„æµ‹æŒ‰é’®
      if st.button("ç”Ÿæˆç¦»èŒé£é™©é¢„æµ‹"):
          prompt = prompt_template3.format(
              analysis_id=analysis_id,
              job_name=job_name,
              analysis_result=json.dumps(analy_json, ensure_ascii=False)
          )
          resp = call_gpt_model(prompt, api_name, api_url, api_key)
          try:
              data = parse_model_output(resp)
              save_retention(ph, resume_id, analysis_id, data.get("risk_level", "æœªçŸ¥"), data)
              st.success("å·²ç”Ÿæˆå¹¶ä¿å­˜ç¦»èŒé£é™©é¢„æµ‹ã€‚")
              st.rerun()
          except Exception as e:
              st.error(f"ç¦»èŒé£é™©é¢„æµ‹è§£æå¤±è´¥: {e}")

